{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7455a80-0245-4f8e-bb01-d882c54b8c24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "126f2344-0dbe-4c13-be20-5c18594285dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install ydata-profiling\n",
    "%pip install --upgrade Pillow\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7506187-3249-4b78-9d7d-fefa1d6ead90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621ed34c-f4ff-483f-ba17-171ca0232c35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Read from source: online file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e2fcb15-bc9a-448b-884e-06ee7731a159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "import urllib\n",
    "\n",
    "DATASET_FILE = \"ts-spark_ch2_ds2.csv\"\n",
    "DATASET_URL = f\"https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch2/{DATASET_FILE}\"\n",
    "\n",
    "# Read from a csv file\n",
    "print(f\"Ingesting from: {DATASET_URL}\")\n",
    "\n",
    "# option 1 - using sparkContext\n",
    "#spark.sparkContext.addFile(DATASET_URL)\n",
    "#df = spark.read.csv(\"file:///\" + SparkFiles.get(DATASET_FILE), header=True, sep=\";\", inferSchema=True)\n",
    "# option 2 - using urllib\n",
    "urllib.request.urlretrieve(DATASET_URL, f\"/tmp/{DATASET_FILE}\")\n",
    "df = spark.read.csv(f\"file:/tmp/{DATASET_FILE}\", header=True, sep=\";\", inferSchema=True)\n",
    "#\n",
    "\n",
    "df.cache()\n",
    "df.count()\n",
    "\n",
    "print(\"Ingestion result:\")\n",
    "df.display()\n",
    "print(\"Inferred schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8eff665-2114-4ba5-99d2-bc3c8b85fb30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2b4876-71b7-45ae-902c-d56dc93d1d3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Fix Time column\n",
    "df = df.withColumn('Time', F.date_format('Time', 'HH:mm:ss'))\n",
    "# Create timestamp column\n",
    "df = df.withColumn('timestamp', F.concat(df.Date, F.lit(\" \"), df.Time))\n",
    "df = df.withColumn('timestamp', F.to_timestamp(df.timestamp, 'yyyy-MM-dd HH:mm:ss'))\n",
    "# Fix data types\n",
    "#df = df.dropna() \\\n",
    "df = df \\\n",
    "    .withColumn('Global_active_power', df.Global_active_power.cast('double')) \\\n",
    "    .withColumn('Global_reactive_power', df.Global_reactive_power.cast('double')) \\\n",
    "    .withColumn('Voltage', df.Voltage.cast('double')) \\\n",
    "    .withColumn('Global_intensity', df.Global_intensity.cast('double')) \\\n",
    "    .withColumn('Sub_metering_1', df.Sub_metering_1.cast('double')) \\\n",
    "    .withColumn('Sub_metering_2', df.Sub_metering_2.cast('double')) \\\n",
    "    .withColumn('Sub_metering_3', df.Sub_metering_3.cast('double'))  \n",
    "\n",
    "print(\"Ingestion result - after initial prep:\")\n",
    "df.display()\n",
    "print(\"Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8a2039-1ed0-49da-a4e2-0fa242359ec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Statistical Summary and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5100dd6e-2b52-4ada-b222-d30d6e6017af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372ad14b-c7cd-444c-8ea2-d5b0d0c6e1e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.summary().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f399b7f7-3877-4882-b109-5eba5f420794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c0c289b-f0c7-41d5-80fa-0d3a56c51f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Fix Date, Time columns for ydata correlations calc\n",
    "df = df.withColumn('Date', F.to_timestamp(df.Date, 'yyyy-MM-dd'))\n",
    "df = df.withColumn('Time', F.to_timestamp(df.Time, 'HH:mm:ss'))\n",
    "\n",
    "df.display()\n",
    "\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# Perform data profiling\n",
    "profile = ProfileReport(pdf,\n",
    "                title='Time Series Data Profiling',\n",
    "                tsmode=True,\n",
    "                sortby='timestamp',\n",
    "                infer_dtypes=False,\n",
    "                interactions=None,\n",
    "                missing_diagrams=None,\n",
    "                correlations={\"auto\": {\"calculate\": False},\n",
    "                              \"pearson\": {\"calculate\": True},\n",
    "                              \"spearman\": {\"calculate\": True}})\n",
    "\n",
    "# Save the profiling report to an HTML file\n",
    "#profile.to_file(\"time_series_data_profiling_report.html\")\n",
    "\n",
    "# Show the profiling report in the notebook\n",
    "report_html = profile.to_html()\n",
    "displayHTML(report_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "502cc421-1a1c-441a-9d5a-318e9a6971a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Gap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9e0364e-08e6-4647-9bc8-b7e65bf83461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# add some random gaps\n",
    "length = pdf.shape[0]\n",
    "to_drop = np.unique(np.sort(np.random.randint(0, length, size=3))).tolist()\n",
    "pdf = pdf.drop(to_drop).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bce2d623-32b2-4c84-85a8-1a8c94aefec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "# test for gaps\n",
    "pdf['gap_val'] = pdf['timestamp'].sort_values().diff()\n",
    "pdf['gap'] = pdf['gap_val'] > ps.to_timedelta('1 minute')\n",
    "pdf[pdf.gap]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd142b78-88f7-40d6-be5f-3fc70d3d5928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccaffad1-2c4a-486f-b8dc-0bba1acd3ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# Extract day and hour\n",
    "df = df.withColumn(\"dayOfWeek\", F.dayofweek(F.col(\"timestamp\")))\n",
    "df = df.withColumn(\"hour\", F.hour(F.col(\"timestamp\")))\n",
    "\n",
    "# Show the data\n",
    "df.display()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# Distribution analysis using Seaborn and Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(pdf['Global_active_power'], kde=True, bins=30)\n",
    "plt.title('Distribution of Global_active_power in Time Series Data')\n",
    "plt.xlabel('Global_active_power')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot to visualize the distribution per dayOfWeek\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='dayOfWeek', y='Global_active_power', data=pdf)\n",
    "plt.title('Daily Distribution of Global_active_power in Time Series Data')\n",
    "plt.xlabel('DayOfWeek')\n",
    "plt.ylabel('Global_active_power')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot to visualize the distribution per hour\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='hour', y='Global_active_power', data=pdf)\n",
    "plt.title('Hourly Distribution of Global_active_power in Time Series Data')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Global_active_power')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3238ef9-2120-4269-babe-d4c2822cf997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9d6219-f729-402b-bf4d-a55a3ea8a4d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df_ = df['timestamp', 'Global_active_power'].toPandas()\n",
    "fig = px.line(df_, x=df_['timestamp'], y=df_['Global_active_power'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fb8fb54-4f32-4352-839f-d2561c7ac42a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Trend, Seasonality and Stationarity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f74809-a15f-45e7-a69c-bc3ae85216af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Resampling and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2173bfe9-71d9-41f6-a0a9-9ac65b1c0caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Convert to Pandas DataFrame for resampling and aggregation\n",
    "pdf = df['timestamp', 'Global_active_power'].toPandas()\n",
    "\n",
    "# Set date as index\n",
    "pdf.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Resample data to hourly, daily and weekly frequency and aggregate by mean\n",
    "hourly_resampled = pdf.resample('h').mean()\n",
    "hourly_resampled_s = pdf.resample('h').std()\n",
    "daily_resampled = pdf.resample('d').mean()\n",
    "daily_resampled_s = pdf.resample('d').std()\n",
    "weekly_resampled = pdf.resample('w').mean()\n",
    "weekly_resampled_s = pdf.resample('w').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d86d8b0c-f14d-4f91-8a6a-6eddfdab4d76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ = hourly_resampled.reset_index()\n",
    "fig = px.line(df_, x=df_['timestamp'], y=df_['Global_active_power'], title=\"Global_active_power - hourly mean\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc404705-fc37-47c0-bd7c-de4351cc7c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ = hourly_resampled_s.reset_index()\n",
    "fig = px.line(df_, x=df_['timestamp'], y=df_['Global_active_power'], title=\"Global_active_power - hourly stddev\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "423a2884-2a5e-4651-8e1a-21d4c1067f44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ = daily_resampled.reset_index()\n",
    "fig = px.line(df_, x=df_['timestamp'], y=df_['Global_active_power'], title=\"Global_active_power - daily mean\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c6eb237-b1c8-466f-a467-86d73f5b1c5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ = daily_resampled_s.reset_index()\n",
    "fig = px.line(df_, x=df_['timestamp'], y=df_['Global_active_power'], title=\"Global_active_power - daily stddev\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1392f04-ce83-4b69-8f0b-a25fda553af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ = weekly_resampled.reset_index()\n",
    "fig = px.line(df_, x=df_['timestamp'], y=df_['Global_active_power'], title=\"Global_active_power - weekly mean\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f061a0c-3934-4c7e-bf10-4b09a5c3747e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ = weekly_resampled_s.reset_index()\n",
    "fig = px.line(df_, x=df_['timestamp'], y=df_['Global_active_power'], title=\"Global_active_power - weekly stddev\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e65dded0-2a98-4f80-84fa-57309133eda9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3a81ea9-4072-49aa-b786-d0780d432889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Perform seasonal decomposition\n",
    "hourly_result = seasonal_decompose(hourly_resampled['Global_active_power'])\n",
    "daily_result = seasonal_decompose(daily_resampled['Global_active_power'])\n",
    "\n",
    "# Plot the decomposed components\n",
    "_plot = hourly_result.plot()\n",
    "_plot.set_size_inches((15, 8))\n",
    "_plot.tight_layout()\n",
    "plt.show()\n",
    "_plot = daily_result.plot()\n",
    "_plot.set_size_inches((15, 8))\n",
    "_plot.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "204ebc49-a149-4389-9b5d-39aa7fb01227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a081781-1bd4-4c99-af21-a55f8fe3ac98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8642f9ca-a8a4-48a5-add8-59ca181343ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform Augmented Dickey-Fuller test\n",
    "result = adfuller(hourly_resampled)\n",
    "\n",
    "# if Test statistic < Critical Value and p-value < 0.05\n",
    "#   reject the Null hypothesis, time series does not have a unit root\n",
    "#   series is stationary\n",
    "# Extract and print the ADF test results\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print(f'   {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e62cfb-a611-48dd-985c-588fd8968526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform Augmented Dickey-Fuller test\n",
    "result = adfuller(daily_resampled)\n",
    "\n",
    "# if Test statistic < Critical Value and p-value < 0.05\n",
    "#   reject the Null hypothesis, time series does not have a unit root\n",
    "#   series is stationary\n",
    "# Extract and print the ADF test results\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print(f'   {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d273fc1f-0f82-4828-8a94-2d84d914fbdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import kpss\n",
    "import pandas as pd\n",
    "\n",
    "# if Test Statistic > Critical Value and p-value < 0.05\n",
    "#   reject the Null hypothesis\n",
    "#   series is non-stationary\n",
    "print ('Results of KPSS Test:')\n",
    "kpsstest = kpss(hourly_resampled, regression='c', nlags=\"auto\")\n",
    "kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','#Lags Used'])\n",
    "for key,value in kpsstest[3].items():\n",
    "    kpss_output['Critical Value (%s)'%key] = value\n",
    "print (kpss_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2aa9530-f97d-40f8-93bd-3a225327238e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import kpss\n",
    "import pandas as pd\n",
    "\n",
    "# if Test Statistic > Critical Value and p-value < 0.05\n",
    "#   reject the Null hypothesis\n",
    "#   series is non-stationary\n",
    "print ('Results of KPSS Test:')\n",
    "kpsstest = kpss(daily_resampled, regression='c', nlags=\"auto\")\n",
    "kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','#Lags Used'])\n",
    "for key,value in kpsstest[3].items():\n",
    "    kpss_output['Critical Value (%s)'%key] = value\n",
    "print (kpss_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0afbb2fb-bd2f-42df-b607-22d2fcc0dee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "###### Check - another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "745218e6-5d4a-49de-8ad1-0b1f18713470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "DATASET_FILE = \"ts-spark_ch1_ds1.csv\"\n",
    "DATASET_URL = f\"https://raw.githubusercontent.com/PacktPublishing/Time-Series-Analysis-with-Spark/main/ch1/{DATASET_FILE}\"\n",
    "\n",
    "spark.sparkContext.addFile(DATASET_URL)\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///\" + SparkFiles.get(DATASET_FILE))\n",
    "df1.createOrReplaceTempView(\"temperatures\")\n",
    "\n",
    "df2 = spark.sql(\"select to_date(Category) as year, float(`Annual Mean`) as annual_mean from temperatures where Category > '1950'\")\n",
    "df2_pd = df2.toPandas()\n",
    "df2_pd['year'] = ps.to_datetime(df2_pd['year'])\n",
    "#display(df2_pd)\n",
    "\n",
    "fig = px.scatter(df2_pd, x=\"year\", y=\"annual_mean\", trendline=\"ols\", title='Average Temperature - Mauritius (from 1950)')\n",
    "fig.update_traces(mode = 'lines')\n",
    "fig.data[-1].line.color = 'red'\n",
    "fig.data[-1].line.dash = 'dash'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9418033-5add-465b-b0b0-80bdc347c6fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform Augmented Dickey-Fuller test\n",
    "result = adfuller(df2_pd['annual_mean'])\n",
    "\n",
    "# if Test statistic < Critical Value and p-value < 0.05\n",
    "#   reject the Null hypothesis, time series does not have a unit root\n",
    "#   series is stationary\n",
    "# Extract and print the ADF test results\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print(f'   {key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5d2a39-d560-48bd-8888-b6bdc779a4b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ff62da-cdb4-4192-bdf1-58f1deb4eda2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calculate the difference (differencing)\n",
    "window = Window.orderBy(\"year\")\n",
    "df2_ = df2.withColumn(\"annual_mean_diff\", F.col(\"annual_mean\") - F.lag(F.col(\"annual_mean\"), 1).over(window))\n",
    "\n",
    "# Drop the first row with null difference\n",
    "df2_ = df2_.na.drop()\n",
    "\n",
    "# Show the differenced data\n",
    "df2_.display()\n",
    "\n",
    "# Convert to Pandas DataFrame for visualization\n",
    "pdf2 = df2_.toPandas()\n",
    "\n",
    "# Set date as index\n",
    "pdf2.set_index('year', inplace=True)\n",
    "\n",
    "# Plot the original and differenced time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(pdf2.index, pdf2['annual_mean'], marker='o')\n",
    "plt.title('Original Time Series')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('annual_mean')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(pdf2.index, pdf2['annual_mean_diff'], marker='o', color='orange')\n",
    "plt.title('Differenced Time Series')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('differenced value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0544779a-7092-4ea6-9cb0-06bc99fea901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform Augmented Dickey-Fuller test\n",
    "result = adfuller(pdf2['annual_mean_diff'])\n",
    "\n",
    "# if Test statistic < Critical Value and p-value < 0.05\n",
    "#   reject the Null hypothesis, time series does not have a unit root\n",
    "#   series is stationary\n",
    "# Extract and print the ADF test results\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print(f'   {key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "517bc836-05d8-44a5-a49c-5e59dea05b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a5749b7-386e-4aa2-a3d8-cb8c2a1f64a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02c9e0e1-1295-464e-bacb-54d9c787b818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Plot Autocorrelation Function (ACF)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_acf(hourly_resampled['Global_active_power'], lags=3*24)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot Partial Autocorrelation Function (PACF)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_pacf(hourly_resampled['Global_active_power'], lags=3*24)\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb982760-070a-4b67-b9bc-81fbb1adce00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Plot Autocorrelation Function (ACF)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_acf(daily_resampled['Global_active_power'], lags=2*7)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot Partial Autocorrelation Function (PACF)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_pacf(daily_resampled['Global_active_power'], lags=2*7)\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f469ac7-7da3-45ac-ad90-4c3fb50172ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Convert to Pandas DataFrame for autocorrelation checks\n",
    "pdf2 = df2_.toPandas()\n",
    "\n",
    "# Set date as index\n",
    "pdf2.set_index('year', inplace=True)\n",
    "\n",
    "# Plot Autocorrelation Function (ACF)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_acf(pdf2['annual_mean'], lags=30)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot Partial Autocorrelation Function (PACF)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_pacf(pdf2['annual_mean'], lags=30)\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231f9538-e586-4ee7-843b-b12c3781a8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Lag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad9cdfa-72d6-4408-80fb-0928b56eff76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "hourly_df = hourly_resampled.reset_index()\n",
    "hourly_df = spark.createDataFrame(hourly_df)\n",
    "\n",
    "# Define window specification\n",
    "window = Window.orderBy(\"timestamp\")\n",
    "\n",
    "# Create lagged features\n",
    "hourly_df = hourly_df.withColumn(\"lag1\", F.lag(F.col(\"Global_active_power\"), 1).over(window))\n",
    "hourly_df = hourly_df.withColumn(\"lag2\", F.lag(F.col(\"Global_active_power\"), 2).over(window))\n",
    "hourly_df = hourly_df.withColumn(\"lag12\", F.lag(F.col(\"Global_active_power\"), 12).over(window))\n",
    "hourly_df = hourly_df.withColumn(\"lag24\", F.lag(F.col(\"Global_active_power\"), 24).over(window))\n",
    "\n",
    "# Show the DataFrame with lagged features\n",
    "hourly_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8dd842c-fc33-46ac-a88f-3a94d5395b83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "# Calculate autocorrelation for lag 1\n",
    "df_lag1 = hourly_df.dropna(subset=[\"lag1\"])\n",
    "autocorr_lag1 = df_lag1.stat.corr(\"Global_active_power\", \"lag1\")\n",
    "\n",
    "# Calculate autocorrelation for lag 2\n",
    "df_lag2 = hourly_df.dropna(subset=[\"lag2\"])\n",
    "autocorr_lag2 = df_lag2.stat.corr(\"Global_active_power\", \"lag2\")\n",
    "\n",
    "# Calculate autocorrelation for lag 12\n",
    "df_lag12 = hourly_df.dropna(subset=[\"lag12\"])\n",
    "autocorr_lag12 = df_lag12.stat.corr(\"Global_active_power\", \"lag12\")\n",
    "\n",
    "# Calculate autocorrelation for lag 24\n",
    "df_lag24 = hourly_df.dropna(subset=[\"lag24\"])\n",
    "autocorr_lag24 = df_lag24.stat.corr(\"Global_active_power\", \"lag24\")\n",
    "\n",
    "print(f\"Autocorrelation for lag 1: {autocorr_lag1}\")\n",
    "print(f\"Autocorrelation for lag 2: {autocorr_lag2}\")\n",
    "print(f\"Autocorrelation for lag 12: {autocorr_lag12}\")\n",
    "print(f\"Autocorrelation for lag 24: {autocorr_lag24}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b237066d-b064-4a2b-aea2-97dd5c95d55c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to Pandas DataFrame for visualization\n",
    "pdf = hourly_df.toPandas()\n",
    "\n",
    "# Drop rows with NaN values (introduced by lagging)\n",
    "pdf = pdf.dropna()\n",
    "\n",
    "# Plot original vs lagged values\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(pdf['timestamp'], pdf['Global_active_power'])\n",
    "plt.plot(pdf['timestamp'], pdf['lag1'])\n",
    "plt.title('Lag 1 vs Original')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(pdf['timestamp'], pdf['Global_active_power'])\n",
    "plt.plot(pdf['timestamp'], pdf['lag2'])\n",
    "plt.title('Lag 2 vs Original')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(pdf['timestamp'], pdf['Global_active_power'])\n",
    "plt.plot(pdf['timestamp'], pdf['lag12'])\n",
    "plt.title('Lag 12 vs Original')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(pdf['timestamp'], pdf['Global_active_power'])\n",
    "plt.plot(pdf['timestamp'], pdf['lag24'])\n",
    "plt.title('Lag 24 vs Original')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adedc548-6c66-4191-a685-414d459085f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Cross-correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3bb344-22fe-4730-9278-5eb861279ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Convert to Pandas DataFrame for resampling and aggregation\n",
    "pdf = df['timestamp', 'Global_active_power', 'Voltage'].toPandas()\n",
    "\n",
    "# Set date as index\n",
    "pdf.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Resample data to hourly frequency and aggregate by mean\n",
    "hourly_resampled = pdf.resample('h').mean()\n",
    "\n",
    "hourly_df = spark.createDataFrame(hourly_resampled)\n",
    "\n",
    "# Compute cross-correlation between value1 and value2\n",
    "cross_corr = hourly_df.stat.corr(\"Global_active_power\", \"Voltage\")\n",
    "\n",
    "print(f\"Cross-correlation between Global_active_power and Voltage: {cross_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfad8772-2f94-49cb-b2ec-ca063879696d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "\n",
    "hourly_ = hourly_resampled.iloc[:36]\n",
    "\n",
    "# Calculate cross-correlation function\n",
    "ccf_values = ccf(hourly_['Global_active_power'], hourly_['Voltage'])\n",
    "\n",
    "# Plot cross-correlation function\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.stem(range(len(ccf_values)), ccf_values, use_line_collection=True, markerfmt=\"-\")\n",
    "plt.title('Cross-Correlation Function (CCF)')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Cross-Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8511c9-9218-4566-a2d0-8860770fa99f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "hourly_df = hourly_resampled.reset_index()\n",
    "hourly_df = spark.createDataFrame(hourly_df)\n",
    "\n",
    "# Define window specification\n",
    "window = Window.orderBy(\"timestamp\")\n",
    "\n",
    "# Create lagged features for value2\n",
    "hourly_df = hourly_df.withColumn(\"Voltage_lag1\", F.lag(F.col(\"Voltage\"), 1).over(window))\n",
    "hourly_df = hourly_df.withColumn(\"Voltage_lag2\", F.lag(F.col(\"Voltage\"), 2).over(window))\n",
    "hourly_df = hourly_df.withColumn(\"Voltage_lag12\", F.lag(F.col(\"Voltage\"), 12).over(window))\n",
    "hourly_df = hourly_df.withColumn(\"Voltage_lag24\", F.lag(F.col(\"Voltage\"), 24).over(window))\n",
    "\n",
    "# Drop rows with null values\n",
    "hourly_df = hourly_df.dropna()\n",
    "\n",
    "# Compute cross-correlation for lagged values\n",
    "cross_corr_lag1 = hourly_df.stat.corr(\"Global_active_power\", \"Voltage_lag1\")\n",
    "cross_corr_lag2 = hourly_df.stat.corr(\"Global_active_power\", \"Voltage_lag2\")\n",
    "cross_corr_lag12 = hourly_df.stat.corr(\"Global_active_power\", \"Voltage_lag12\")\n",
    "cross_corr_lag24 = hourly_df.stat.corr(\"Global_active_power\", \"Voltage_lag24\")\n",
    "\n",
    "print(f\"Cross-correlation between Global_active_power and Voltage (lag 1): {cross_corr_lag1}\")\n",
    "print(f\"Cross-correlation between Global_active_power and Voltage (lag 2): {cross_corr_lag2}\")\n",
    "print(f\"Cross-correlation between Global_active_power and Voltage (lag 12): {cross_corr_lag12}\")\n",
    "print(f\"Cross-correlation between Global_active_power and Voltage (lag 24): {cross_corr_lag24}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ts-spark_ch6_1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}